MCP Mesh: The New Way to See the World

*Before we dive in, a bit of context: I spent several years working at Mercado Libre, where I witnessed firsthand the evolution from monolith chaos to platform-driven development. That experience shaped much of how I think about scalable systems today â€” and why I believe we're about to relive a very similar journey with AI.*

The Microservices DÃ©jÃ  Vu
Back in 2016, as microservices adoption started to grow, companies discovered both the benefits and the challenges of this new architecture. More flexibility and speed â€” but also higher technical complexity, multi-cloud management, the risk of someone unintentionally impacting other services, and rising operational costs.
Mercado Libre was one of the companies that faced these challenges at scale. Their monolithic architecture could no longer keep up, and coordinating thousands of developers was becoming increasingly difficult.
Fast forward to today, and we're seeing history repeat itself â€” this time with AI.
Companies are juggling countless ChatGPT tabs, scattered API keys, and AI agents that can't communicate with each other. Each team builds its own isolated solution, context gets lost, governance is minimal. It's inefficient â€” but all too common.
The solution that helped Mercado Libre overcome these challenges â€” the Fury platform â€” provides an interesting blueprint for solving the scaling issues companies are now facing with AI.
And that blueprint has a name: Deco.
Let me show you why this matters for you.

From Chaos to Governance: The Mercado Libre Story
In an article by Juliano Martins from Mercado Libre, he explains how MELI turned the challenges of microservices into a powerful platform that allows developers to build fast, deploy safely, and scale indefinitely.
They went through three distinct stages:
Old World
Mercado Libre began with a large monolithic system running on a few servers and a single database. With hundreds of developers working in the same repository, weekly deployments, and complex environments, conflicts were frequent and scalability was highly limited.
Explicit New World
Starting in 2010, the company migrated to a microservices architecture with MeliCloud. Infrastructure as a service brought autonomy and speed, enabling thousands of instances, hundreds of traffic pools, and more than 1,400 daily deployments.
Implicit New World
But freedom came with a cost. The microservices era introduced a new kind of chaos: complex environments, high operational overhead, and the need for deep infrastructure knowledge. Teams spent too much time dealing with incidents instead of improving products.
The Birth of Fury
To simplify engineers' operational work, Mercado Libre centralized everything â€” infrastructure, build, deployment, metrics, and services â€” while keeping frontends and APIs clearly separated. The vision was simple: let developers run an application in just a few clicks, easy to modify and test.
This led to the creation of Fury, a platform designed to make development and production management far more efficient.
Today, MELI performs around 30,000 deployments per day â€” made possible by the tooling, governance, and centralized platform they built.

MCP Mesh
The journey Mercado Libre went through â€” from the initial chaos of microservices to the creation of the Fury platform â€” mirrors exactly what companies are experiencing today with AI.
Back then, the challenge was scaling, autonomy, and governance of infrastructure.
Today, the challenge is scaling, autonomy, and governance of AI contexts and agents.
Just as in the early microservices era:
Each team built its own isolated solutions
The environment fragmented
Complexity exploded
Duplication multiplied
Lack of standardization drove up incidents and operational costs
With AI, we see the same pattern emerging:
Isolated agents
Scattered data across countless systems
Duplicated integrations
Zero visibility into usage
No governance over who accesses what
Enter MCP (Model Context Protocol)
If Fury brought order to the chaos of microservices, MCP is the first step toward bringing order to the chaos of AI agents.
Fury standardized how services communicate, deploy, and operate.
MCP standardizes how AI agents access data, tools, and APIs.
Fury was the protocol that organized infrastructure for humans.
MCP is the protocol that organizes context for machines.
MCP creates the foundation for a new era: AI that can truly understand, retrieve, and interact with your resources in a consistent, governed, and reusable way.
But Here's the Catch
Just like microservices, the initial freedom quickly leads to a new type of chaos. As every team begins building dozens of agents using dozens of MCPs, companies hit the same fragmentation wall that MELI faced before Fury.
This is exactly where the concept of the MCP Mesh emerges â€” the "Fury moment" for AI, providing the governance, standardization, observability, and scale required to run AI across an entire organization.
A standardized way for AI to access:
Data
Tools
APIs
Resources
That means your agent can access your data with your permission, and interact with it intelligently.

The Power of Connected AI: A Real Example
AI Sales & Opportunity Scout
What if an AI could scan the market, competitors, client behavior, and trends â€” and instantly highlight sales opportunities you didn't even know existed?
Not just numbers, but insights, summaries, and actionable recommendations: which leads to prioritize, which clients might churn, and where to focus cross-sell efforts.
Now imagine connecting this agent to:
- LinkedIn â€” monitoring posts, comments, and client feedback to detect complaints or opportunities early
- Your CRM â€” gathering customer context automatically
- Email â€” surfacing relevant conversations
- Industry news feeds â€” tracking market movements in real time
Now Scale That Vision
What if you had ten more agents, each handling different tasks: analyzing client feedback, monitoring trends, or optimizing outreach?
What if every department in your company had its own set of agents, all working in sync, connected to the right contexts, securely using tokens, and constantly coordinating to avoid duplication?
Sounds powerful â€” but then the question becomes:
> How do you manage all these agents efficiently?
> How do you control the contexts and MCPs they use?
> How do you handle costs, prevent overlap, and keep everything organized and secure?

MCP Mesh: Your Entire Ecosystem Under One Context Orchestration
A distributed network of MCP servers forming a unified context gateway.
AI apps connect to the mesh â€” not to 15 different systems.
The Mesh Handles:
Routing â€” Direct requests to the right MCP
Caching â€” Reduce redundant calls and costs
Authentication â€” Secure access control
Composition â€” Combine multiple contexts seamlessly
Governance â€” Policies, quotas, and compliance
Without a Mesh:
âŒ Costs explode
âŒ Shadow AI spreads
âŒ Integrations duplicate
âŒ Compliance becomes impossible
âŒ Observability vanishes
Companies hit a wall around 10â€“15 AI applications.

The MCP Mesh Revolution Has Already Begun
What defines the winners of this cycle isn't who "uses more AI" â€” it's who builds the platform that makes AI secure, governed, observable, and scalable.
And that's exactly where Deco comes in.
While the market races toward closed, proprietary, and expensive solutions, Deco is born with a radically different vision:
> Bringing the future of distributed AI to every company with an open-source, self-hosted, mesh-first platform.
You can finally unlock the real power of AI â€” because context is power.

Why Deco
Deco delivers what the market needs â€” and what no proprietary solution is offering:
A Complete Mesh of MCPs
âœ… Plug-and-play connections
âœ… Real reuse across teams
âœ… Unified internal catalog
âœ… Native observability
âœ… Integrated security
âœ… Deep standardization
âœ… Zero vendor lock-in
Governance and Control Without Friction
âœ… Centralized audit
âœ… Team and domain policies
âœ… Smart quotas and limits
âœ… Full cost visibility
âœ… End-to-end MCP versioning
âœ… SSO, RBAC, enterprise compliance
A Great Developer Experience
Deco creates the platform to scale AI:
Building an MCP is fast â€” for tech and non-tech users
Publishing is safe
Consuming is simple
Scaling is automatic
Management is transparent
You Don't Need to Build an Internal Mesh â€” Deco Ships With One
While other platforms force you to assemble complex, expensive, irreversible infrastructure, Deco gives you:
âœ… Functional mesh
âœ… Open-source
âœ… Self-hosted
âœ… Extensible
âœ… Governed
âœ… Secure
âœ… Standardized
From day one.

Conclusion: From Prompt-Chaos to MCP Mesh
The world went through the challenges of microservices. Now we're going through the challenges of context and MCPs.
Before vs. After
Old World (Chaos) â†’ Implicit New World (MCP Mesh)
âŒ 50+ ChatGPT tabs â†’ âœ… A single governed MCP mesh
âŒ API keys everywhere â†’ âœ… Centralized auth and routing
âŒ Zero spend visibility â†’ âœ… Full observability
âŒ Reinventing the wheel â†’ âœ… Shared context
âŒ No audit trail â†’ âœ… Automatic compliance
âŒ Fragmented tools â†’ âœ… Your context everywhere
This Is the Evolution Every Company Needs
With Deco, your company:
âš¡ Accelerates AI development
ğŸ”„ Reduces duplication
ğŸš« Eliminates shadow AI
ğŸ”’ Increases security
ğŸ“Š Gains real governance
ğŸ”— Standardizes integrations
ğŸ‘¥ Distributes AI to every team
ğŸ’° Saves time and money

The Journey
Old World (prompt-chaos) â†’ Explicit New World (MCP) â†’ Implicit New World (MCP Mesh)
Because in the end, the winners aren't defined by the best model â€” but by the best context.


Prompt para Pull Request no TanStack DB
# Feature Request: Support for Updating Items from onInsert/onUpdate/onDelete Handlers
## Problem Statement
Currently, TanStack DB's transaction system ignores the return values from `onInsert`, `onUpdate`, and `onDelete` handlers. This creates issues when the backend modifies data during persistence (e.g., generating real IDs, adding timestamps, computing fields).
**Current Behavior:**
// In transactions.ts around line 491-498
await this.mutationFn({
  transaction: this as unknown as TransactionWithMutations<T>,
})
this.setState(`completed`)
this.touchCollection()
this.isPersisted.resolve(this)
// âŒ Return value is discarded!**Real-World Impact:**
- Collections use temporary IDs (e.g., `temp_abc123`) that never get replaced with real backend IDs (e.g., `conn_xyz789`)
- Backend-computed fields (timestamps, slugs, etc.) don't appear in the collection
- Optimistic updates show stale data even after successful persistence
- Queries using the real ID fail because the collection still has the temporary ID
## Desired Behavior
The handlers should be able to return updated items that replace the optimistically-added items in the collection:
const collection = createCollection({
  onInsert: async ({ transaction }) => {
    const results = await Promise.all(
      transaction.mutations.map(async ({ modified: data }) => {
        const response = await api.create(data);
        return response.item; // âœ… This should update the collection!
      })
    );
    return results; // Return updated items with real IDs
  },
});
// Usage
const tx = collection.insert({ id: "temp_123", title: "Test" });
await tx.isPersisted.promise;
// Expected: Collection now has { id: "conn_real", title: "Test", created_at: "..." }
// Actual: Collection still has { id: "temp_123", title: "Test" }## Technical Requirements
### 1. **Handler Return Type Support**
Update the handler signatures to support returning updated items:
type InsertHandler<T> = (context: {
  transaction: Transaction<T>;
  collection: Collection<T>;
}) => Promise<T[] | void>; // Allow returning updated items
type UpdateHandler<T> = (context: {
  transaction: Transaction<T>;
  collection: Collection<T>;
}) => Promise<T[] | void>; // Allow returning updated items
type DeleteHandler<T> = (context: {
  transaction: Transaction<T>;
  collection: Collection<T>;
}) => Promise<T[] | void>; // Allow returning deleted items (for confirmation)### 2. **Transaction Execution Flow**
Modify `transactions.ts` to capture and apply returned items:
// Around line 491-498, instead of:
await this.mutationFn({
  transaction: this as unknown as TransactionWithMutations<T>,
})
// Do something like:
const updatedItems = await this.mutationFn({
  transaction: this as unknown as TransactionWithMutations<T>,
})
if (updatedItems && Array.isArray(updatedItems)) {
  // Apply updates to the collection
  this.applyBackendUpdates(updatedItems)
}### 3. **Handle ID Changes**
Critical: When a backend changes an item's ID, we need to:
- Remove the old item (with temp ID) from the collection
- Insert the new item (with real ID) into the collection
- Maintain all indexes correctly
- Notify subscribers of the change
private applyBackendUpdates(updatedItems: T[]) {
  this.collection.beginBatch()
  
  for (let i = 0; i < this.mutations.length; i++) {
    const mutation = this.mutations[i]
    const updatedItem = updatedItems[i]
    
    if (!updatedItem) continue
    
    const oldKey = mutation.key
    const newKey = this.collection.getKey(updatedItem)
    
    if (oldKey !== newKey) {
      // ID changed - remove old, insert new
      this.collection.deleteByKey(oldKey)
      this.collection.insertItem(updatedItem)
    } else {
      // ID same - just update
      this.collection.updateByKey(oldKey, updatedItem)
    }
  }
  
  this.collection.commitBatch()
}### 4. **Backward Compatibility**
- Handlers that return `void` or `undefined` should work as before (no updates)
- Handlers that return items should trigger the update logic
- Add configuration option to enable/disable this behavior if needed
### 5. **Edge Cases to Handle**
- âœ… Handler returns fewer items than mutations (some failed?)
- âœ… Handler returns items in different order than mutations
- âœ… Handler returns `null` or `undefined` for specific items (skip update)
- âœ… Concurrent mutations during the update
- âœ… Collection is destroyed during transaction
## Files to Modify
Based on the TanStack DB v0.5.10 structure:
1. **`src/types.ts`** - Update handler type definitions
2. **`src/collection/transactions.ts`** - Capture and apply handler returns (around line 491-498)
3. **`src/collection/index.ts`** - Add batch update methods if needed
4. **`docs/`** - Update documentation with examples
## Success Criteria
- [ ] Handlers can return updated items that replace optimistic items
- [ ] ID changes are handled correctly (remove old key, insert with new key)
- [ ] All indexes are updated properly
- [ ] Subscribers receive notifications about the changes
- [ ] Backward compatible with existing code
- [ ] Test coverage for the new behavior
- [ ] Documentation updated with examples
## Example Test Case
test('onInsert updates items with backend response', async () => {
  const collection = createCollection({
    getKey: (item) => item.id,
    onInsert: async ({ transaction }) => {
      // Simulate backend changing temp ID to real ID
      return transaction.mutations.map(({ modified }) => ({
        ...modified,
        id: modified.id.replace('temp_', 'real_'),
        created_at: new Date().toISOString(),
      }))
    },
  })
  const tx = collection.insert({ id: 'temp_123', title: 'Test' })
  await tx.isPersisted.promise
  // Should have real ID now
  const item = collection.get('real_123')
  expect(item).toBeDefined()
  expect(item.id).toBe('real_123')
  expect(item.created_at).toBeDefined()
  
  // Old temp ID should be gone
  expect(collection.get('temp_123')).toBeUndefined()
})## Additional Context
This feature is essential for real-world applications where:
- Backend generates IDs (UUIDs, nanoid, database sequences)
- Backend adds audit fields (timestamps, user info)
- Backend normalizes/validates data
- Backend computes derived fields
Without this, developers must:
- Manually trigger re-sync after mutations (slow, race conditions)
- Use workarounds like manual updates (error-prone)
- Avoid optimistic updates entirely (poor UX)
Please implement this feature to make TanStack DB more suitable for production applications with backend persistence.
typescript// In transactions.ts around line 491-498await this.mutationFn({transaction: this as unknown as TransactionWithMutations<T>,})this.setState(completed)this.touchCollection()this.isPersisted.resolve(this)// âŒ Return value is discarded!
**Real-World Impact:**
- Collections use temporary IDs (e.g., `temp_abc123`) that never get replaced with real backend IDs (e.g., `conn_xyz789`)
- Backend-computed fields (timestamps, slugs, etc.) don't appear in the collection
- Optimistic updates show stale data even after successful persistence
- Queries using the real ID fail because the collection still has the temporary ID
## Desired Behavior
The handlers should be able to return updated items that replace the optimistically-added items in the collection:
const collection = createCollection({
  onInsert: async ({ transaction }) => {
    const results = await Promise.all(
      transaction.mutations.map(async ({ modified: data }) => {
        const response = await api.create(data);
        return response.item; // âœ… This should update the collection!
      })
    );
    return results; // Return updated items with real IDs
  },
});
// Usage
const tx = collection.insert({ id: "temp_123", title: "Test" });
await tx.isPersisted.promise;
// Expected: Collection now has { id: "conn_real", title: "Test", created_at: "..." }
// Actual: Collection still has { id: "temp_123", title: "Test" }
Technical Requirements
1. Handler Return Type Support
Update the handler signatures to support returning updated items:
type InsertHandler<T> = (context: {
  transaction: Transaction<T>;
  collection: Collection<T>;
}) => Promise<T[] | void>; // Allow returning updated items
type UpdateHandler<T> = (context: {
  transaction: Transaction<T>;
  collection: Collection<T>;
}) => Promise<T[] | void>; // Allow returning updated items
type DeleteHandler<T> = (context: {
  transaction: Transaction<T>;
  collection: Collection<T>;
}) => Promise<T[] | void>; // Allow returning deleted items (for confirmation)
2. Transaction Execution Flow
Modify transactions.ts to capture and apply returned items:
// Around line 491-498, instead of:
await this.mutationFn({
  transaction: this as unknown as TransactionWithMutations<T>,
})
// Do something like:
const updatedItems = await this.mutationFn({
  transaction: this as unknown as TransactionWithMutations<T>,
})
if (updatedItems && Array.isArray(updatedItems)) {
  // Apply updates to the collection
  this.applyBackendUpdates(updatedItems)
}
3. Handle ID Changes
Critical: When a backend changes an item's ID, we need to:
Remove the old item (with temp ID) from the collection
Insert the new item (with real ID) into the collection
Maintain all indexes correctly
Notify subscribers of the change
private applyBackendUpdates(updatedItems: T[]) {
  this.collection.beginBatch()
  
  for (let i = 0; i < this.mutations.length; i++) {
    const mutation = this.mutations[i]
    const updatedItem = updatedItems[i]
    
    if (!updatedItem) continue
    
    const oldKey = mutation.key
    const newKey = this.collection.getKey(updatedItem)
    
    if (oldKey !== newKey) {
      // ID changed - remove old, insert new
      this.collection.deleteByKey(oldKey)
      this.collection.insertItem(updatedItem)
    } else {
      // ID same - just update
      this.collection.updateByKey(oldKey, updatedItem)
    }
  }
  
  this.collection.commitBatch()
}
4. Backward Compatibility
Handlers that return void or undefined should work as before (no updates)
Handlers that return items should trigger the update logic
Add configuration option to enable/disable this behavior if needed
5. Edge Cases to Handle
âœ… Handler returns fewer items than mutations (some failed?)
âœ… Handler returns items in different order than mutations
âœ… Handler returns null or undefined for specific items (skip update)
âœ… Concurrent mutations during the update
âœ… Collection is destroyed during transaction
Files to Modify
Based on the TanStack DB v0.5.10 structure:
src/types.ts - Update handler type definitions
src/collection/transactions.ts - Capture and apply handler returns (around line 491-498)
src/collection/index.ts - Add batch update methods if needed
docs/ - Update documentation with examples
Success Criteria
[ ] Handlers can return updated items that replace optimistic items
[ ] ID changes are handled correctly (remove old key, insert with new key)
[ ] All indexes are updated properly
[ ] Subscribers receive notifications about the changes
[ ] Backward compatible with existing code
[ ] Test coverage for the new behavior
[ ] Documentation updated with examples
Example Test Case
test('onInsert updates items with backend response', async () => {
  const collection = createCollection({
    getKey: (item) => item.id,
    onInsert: async ({ transaction }) => {
      // Simulate backend changing temp ID to real ID
      return transaction.mutations.map(({ modified }) => ({
        ...modified,
        id: modified.id.replace('temp_', 'real_'),
        created_at: new Date().toISOString(),
      }))
    },
  })
  const tx = collection.insert({ id: 'temp_123', title: 'Test' })
  await tx.isPersisted.promise
  // Should have real ID now
  const item = collection.get('real_123')
  expect(item).toBeDefined()
  expect(item.id).toBe('real_123')
  expect(item.created_at).toBeDefined()
  
  // Old temp ID should be gone
  expect(collection.get('temp_123')).toBeUndefined()
})
Additional Context
This feature is essential for real-world applications where:
Backend generates IDs (UUIDs, nanoid, database sequences)
Backend adds audit fields (timestamps, user info)
Backend normalizes/validates data
Backend computes derived fields
Without this, developers must:
Manually trigger re-sync after mutations (slow, race conditions)
Use workarounds like manual updates (error-prone)
Avoid optimistic updates entirely (poor UX)
Please implement this feature to make TanStack DB more suitable for production applications with backend persistence.
